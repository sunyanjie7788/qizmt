<wiki:comment>Image:</wiki:comment><img src="http://qizmt.googlecode.com/svn/wiki/images/Qizmt_logo_small.png" alt="Qizmt logo (small)" />

Back to <wiki:comment>Link:</wiki:comment>[Qizmt]

<wiki:toc/>

==Where is the word count example?==
----
Generate the built-in Qizmt examples by issuing the command: *qizmit examples*
<br/>e.g.
{{{qizmit examples}}}
  Qizmt exec Qizmt-GroupBy.xml<br/>
  Qizmt exec Qizmt-WordCount.xml<br/>
  Qizmt exec Qizmt-CellularKMeans.xml <br/>
  Qizmt exec Qizmt-SortInt.xml<br/> 
  Qizmt exec Qizmt-Pointers.xml<br/> 
  Qizmt exec Qizmt-Linq.xml <br/>
  Qizmt exec Qizmt-Geometry.xml<br/>

All of the examples are self-contained; they all generate their own inputs and verify their own outputs and can be executed without any preparation.
<br/><Br/>
The examples can be viewed with the *edit* command.
<br/>
e.g.<br/>
{{{qizmt edit Qizmt-Linq.xml}}}

==When I try to open a text file in MR.DFS with  "qizmt edit WordCount_Output.txt" I get an error.==
----
Currently only support editing job files (the green ones)
But the top of the file can be viewed with the *head* command.<br/>
e.g.<br/>
{{{qizmt head WordCount_Output.txt}}}<br/>
{{{qizmt head WordCount_Output.txt 100}}}   (to view 100 lines etc.)
<br/><br/>
Also, can pull the file out of mr.dfs and view it:<br/><br/>
{{{qizmt get WordCount_Output.txt \\<hostname>\D$\folder\WordCount_Output.txt && notepad \\<hostname>\D$\folder\WordCount_Output.txt}}}
==How do I get a C# function that accessible to a Qizmt mapreducer?==
----
There are a few ways:
 # The C# function, struct or class declaration after “<![CDATA[“ and before  in map() or reduce() clause that will use it.<br/>This will allow for debugging the functionality while it is being executed by the mapreducer.<br/><br/>
 # Put `*`.dll – will put a DLL into MR.DFS and reference it from a mapreducer job with.<br/>e.g.<br/>
{{{
<Job …><Add Reference="regression_test_testdll.dll" Type="dfs" />
}}}

==How to put a file into cluster from a remote machine?==
----
There are a few ways:
 # Use the rput command, this will copy the file locally with the current credentials then put it into qizmt with the qizmt credentials.
 # Copy locally then put into qizmt with put command
 # Write a remote job that opens an ADO.Net connection and streams data in from an RDBMS
==Is byteslice equivalent to a line of text?==
----
By default yes, however a “line” once in mr.dfs is not the same as a line in a text file but is an abstract tuple of data as it can also contain binary data.<br/><br/>
In the case of *binaryput* a line stores an image with a header<br/>
e.g.<br/>
{{{qizmt binaryput \\<hostname>\d$\images\*.png}}}<br/><br/>
Accessed in mapreducer with:
{{{
public virtual void Map(ByteSlice line, MapOutput output)
{
   Blob b = line.ReadBinary();
   string name = b.name;
   byte[] data = b.data;
}
}}}
In the case of newline separated text data, can use put command.<br/>
e.g.<br/>
{{{qizmt put \\<hostname>\d$\images\*.png}}}<br/><br/>
Accessed in mapreducer with:
{{{
public virtual void Map(ByteSlice line, MapOutput output)
{
   mstring sLine = mstring.Prepare(line);
}}}
==Ok, so when I move a file to the cluster, how do I reference it in the xml?==
----
IO tags of job, set the input of the mapreducer to the file in MR.DFS<br/>
e.g.<br/>
{{{
<Job …
  <IOSettings>
    .
    .
    .
   <DFSInput>dfs://OpenCV_Input.blob</DFSInput>
}}}

==How do I create a new job in MR.DFS?==
----
To create a job in MR.DFS, RDP into any machine of the cluster and issue the edit with the name of the jobs file you wish to create.<br/>
e.g. {{{qizmt edit dfs://myjob.xml}}}

If a jobs file by the same name already exists, then the editor will open it for editing, however, if it does not exist, the editor will create an example job which includes 3 major job types: local, remote and mapreduce.
==What is the easiest way to quickly prototype mapreducer logic?==
----
 # Use the edit command to create a new jobs file
  e.g. {{{qizmt edit dfs://myjob.xml}}}
 # Modify the lines of the first remote job to some sample data of what your input will look like
 # Modify the map and reduce logic
 # Save, Close and Execute the jobs file, or execute it in the debugger with F5
==How to sort data with qizmt?==
----
All mapreducer jobs inherently sort data across the cluster. However, there are a few OutputMethods which sort differently:

{{{<OutputMethod>grouped</OutputMethod>}}} - each core in the cluster gets a hash of the keys during the exchange. The data is not fully sorted across the cluster, but each cores hash of the keys are sorted so that all identical-keys make their way to the same reducer.

{{{<OutputMethod>sorted</OutputMethod>}}} - The mapper is executed twice over the input, first to determine distribution of the keys, padding, etc. then again to break the data up into key ranges. During the exchange phase, each machine gets an even distribution of the keys and they keys are evenly sorted across the cluster.  The data is fully sorted across the cluster when the sorted output method is used.

{{{<OutputMethod>hashsorted</OutputMethod>}}} - KeyMajor must be set to 2 to use hashsorted, a 2 byte hashtable of all possible values is used during the exchange phase. The data is fully sorted across the cluster when the hashsorted output method is used.
==Killall does not work in production.==
----
When there is a permission issue on some production clusters killall may not work. In this case can use an alternative way to killall distributed processes in the cluster:

By the way, to killall on production machine is *qizmt killall proxy –f*  then issue *qizmt health* until it says *100% healthy*

==System.Exception:Insufficient resources for this job on cluster (ZBlock value file size `>` ZFILE_MAX_BYTES) (consider increasing sub process count)==
----
Under *IOSettings* of a grouped mapreduce job can add a *Setting* to increase the number of intermediate blocks so that they are smaller and not too much tries to load at once per core:

e.g.
{{{
<IOSettings>
  <Setting name="Subprocess_IntermediateBlockPrime" value="499" />
  ...
</IOSettings>
}}}

The default zblock count is 271, which is how many chunks per core in the cluster a file is broken into for the sort phase.
*Subprocess_IntermediateBlockPrime* must be set to a prime number.

There is a qizmt command *nearprime* which can be used to get a prime  number near some value, e.g.

D:\>*qizmt nearprime* 500

500 is not prime

499 is nearest prime less than 500

503 is nearest prime greater than 500 


Note also, regardless of increasing *Subprocess_IntermediateBlockPrime* count, all identical keys go into the same zblock. If this is the case, can have the key also include a random number between 0 and 100 so that all identical keys are copped into at least 100 different keys.


In the case that it’s too much going to a single key, here is an example of breaking it up with random number in key. But if it is not too much going into a single key, can just increase the *Subprocess_IntermediateBlockPrime* so that less keys get loaded at once into the reducers. 

{{{
<KeyLength>*int*,int,double</KeyLength>
  .
  .
  .
  <![CDATA[
  public Random random = new Random();
  public virtual void Map(ByteSlice line, MapOutput output)
  {
    mstring sLine = mstring.Prepare(line);
    int year = sLine.NextItemToInt(',');
    mstring title = sLine.NextItemToString(',');
    double size = sLine.NextItemToDouble(',');
    long pixel = sLine.NextItemToLong(',');
                
    recordset rKey = recordset.Prepare();
    rKey.PutInt(random.Next(100));
    .
    .
    .
  }
  ]]>
 </Map>
 <Reduce>
  <![CDATA[
  public override void Reduce(ByteSlice key, ByteSliceList values, ReduceOutput output)
  {
    recordset rKey = recordset.Prepare(key);
    int discard_random = rKey.GetInt();
    .
    .
    .
   }
   ]]>
}}}

==I specify key length 16, but my key actually smaller. Should String.Trim() remove spaces?==
----
If key length is 16, then the string can be up to 8 glyphs. If the key is output as string or mstring, it will automatically be right padded 
or glyphs removed until it is at or under keylengh in bytes. Bytes are removed at glyph boundaries so that a glyph is not cut in half. If 
you do not want any string or mstring keys to loose glyps, then `<KeyLength>` needs to be set larger than the largest string that will be 
supported in the key, however the larger the KeyLength the more processing overhead for the exchange phase.

Also note that the text input and output to a mapreducer is UTF8 but the output of Map() is UTF16 and the input to Reduce() is UTF16.

In mapper:
{{{
mstring sMyString = mstring.Prepare(“car”);
output.Add(sMyString);
</source>
In reducer: 
<source lang="csharp">
mstring sMyString = mstring.Prepare(UnpadKey(key));
}}}

UTF8 -`>` Map() -`>` [intermediate data: UTF16] -`>` Reduce() -`>` UTF8

e.g.
{{{
<Job Name="WordCount" Custodian="" email="">
  <IOSettings>
    <JobType>mapreduce</JobType>
    <KeyLength>100</KeyLength>
    .
    .
    .
  </IOSettings>
  <MapReduce>
    <Map>
    <![CDATA[
    public virtual void Map(ByteSlice line, MapOutput output)
    {
      mstring sLine= mstring.Prepare(line);
      output.Add(sLine, mstring.Prepare(1));  // Output automatically right pads strings with \0 or removes from right glyphs until it fits 

the KeyLength
    }
    ]]>
    </Map>

    <Reduce>
    <![CDATA[
    public override void Reduce(ByteSlice key, ByteSliceList values, ReduceOutput output)
    {
      mstring sLine = mstring.Prepare(UnpadKey(key));
      sLine = sLine.AppendM(',').AppendM(values.Length);              
      output.Add(sLine);
    }
    ]]>
    </Reduce>
</MapReduce>
</Job>
}}}

==System.Exception: mstring cannot accommodate your string operations.  Please use C# string or StringBuilder instead.==
----
If doing an extensive amount of appending, then declare a StringBuilder outside of the map() or reduce() and reuse it in-between each 
iteration.

Can just use ToString, no need to use mstring.

The exception is related to out of memory, so on your dev machine it will occur sooner, in production it will be able to grow to the max of {{{byte[]}}} for all mstrings in the current itteration. Can use qizmt ps to see available memory and any other running jobs that may be using memory.

==How to put my own C# DLL available to Qizmt jobs?==
----
If you want to build a C# DLL and reference it, you can build a regular C# dll and use Qizmt put to put it into MR.DFS then reference it in 
the job. Qizmt put automatically treats `*`.dll files differently as other files and makes a redundant copy of the DLL on every machine in the 
cluster. After putting a DLL into a Qizmt cluster , it can be referenced from within the job:
{{{
<Job Name="testdll" Custodian="" email="">
  <Add Reference="Qizmt-AddUsingReferences_testdll.dll" Type="dfs" /> <!- custom C# DLL put into MR.DFS ->
  <Add Reference="Microsoft.VisualBasic.dll" Type="system" /> <!-- system DLL already in .NET -->
  <Using>testdll</Using> <!-- using namespace -->
  <Using>Microsoft.VisualBasic</Using><!-- using namespace -->
  .
  .
  .
</Job>
}}}

{{{qizmt put `*`.dll}}} puts a dll into MR.DFS and makes it available to all job types; local, remote and mapreduce. There are two reference 
types supported *dfs* and *system*.

Can use type *dfs* for custom dlls or dlls that are referenced by custom dlls used in local, remote or mapreducer jobs.

Can use type *system* for referencing system dlls that are not referenced by default. Here are the system dlls that are referenced by 

default:

  System.dll<br/>
  System.Xml.dll<br/>
  System.Data.dll<br/> 
  System.Drawing.dll<br/>
  System.Core.dll<br/>

e.g.
{{{
<Add Reference="regression_test_testdll.dll" Type="dfs" />
<Add Reference="Microsoft.VisualBasic.dll" Type="system" />
<MapReduce>
  <Map>
  <![CDATA[
  public virtual void Map(ByteSlice line, MapOutput output)
  {
    if(32 != Microsoft.VisualBasic.Strings.Asc(' '))
    {
      throw new Exception("Map: (32 != Microsoft.VisualBasic.Strings.Asc(' '))");
    }
    //...
  }
  ]]>
}}}

Also, dlls put into MR.DFS may reference each other.

When a dll is put into MR.DFS it is copied to every machine in the cluster so that all job types can load it locally.

==It looks like my job is hanging. Is there a way to say what happening to it?==
----
If there are errors happening, you can see them early with:

{{{qizmt slaveloglist}}}

can also clear slave logs with:

{{{qizmt slavelogdelete}}}

==How to run multiple remote jobs on different machines processing different files in MR.DFS?==
----
If just 1 remote with 1 DFS_IO then it all goes to 1 machine, but keep in mind that bottlenecks bandwidth to 128 megabyte per second. If 
multiple DFS_IO or mapreducer then your total bandwidth is 128 megabyte per second `*` number of hosts.

Here is example of streaming 4 MR.DFS files to 4 processes. Two of the remotes running on MACHINEA and two of the remotes running on 
MACHINEB.
{{{
<Job Name="multiremotes_InputData" Custodian="" Email="" Description="">
  <IOSettings>
    <JobType>remote</JobType>
    <DFS_IO>
      <DFSReader>dfs://File0.txt</DFSReader>
      <DFSWriter>dfs://File0_output.txt</DFSWriter>
      <Host>MACHINEA</Host>
    </DFS_IO>
    <DFS_IO>
      <DFSReader>dfs://File1.txt</DFSReader>
      <DFSWriter>dfs://File1_output.txt</DFSWriter>
      <Host>MACHINEA</Host>
    </DFS_IO>
    <DFS_IO>
      <DFSReader>dfs://File2.txt</DFSReader>
      <DFSWriter>dfs://File2_output.txt</DFSWriter>
      <Host>MACHINEB</Host>
    </DFS_IO>
    <DFS_IO>
      <DFSReader>dfs://File3.txt</DFSReader>
      <DFSWriter>dfs://File3_output.txt</DFSWriter>
      <Host>MACHINEB</Host>
    </DFS_IO>
  </IOSettings>
  <Remote>
  <![CDATA[
  public virtual void Remote(RemoteInputStream dfsinput, RemoteOutputStream dfsoutput)
  {    
    System.IO.StreamReader sr = new System.IO.StreamReader(dfsinput);
    //
  }
  ]]>
}}}
==Is there a way to read more than one dfs file at the same time? Let’s say I need compare 2 dfs files line by line. I may specify Remote job that process dfsinput, but I need open another dfs stream. Could I do that?==
----
Could do a mapreducer to prepend a category “A,” to the first MR.DFS file, and another mapreducer to prepend “B,” to the second file. Then can run them both into a single mapreducer which keys both on the category and the line. Then in reducer compare the lines. This would remain fully distributed way to compare two files.


Also, there are some high level commands for getting an MD5 or a checksum of a file which could be compared if the entire files are identical but these are not distributed and a bit slow, we just use them for regression testing:


  md5 `<dfsfile>`   compute MD5 of DFS data file<br/>
  checksum `<dfsfile>`   compute sum of DFS data file<br/>
  sorted `<dfsfile>`   check if a DFS data file has sorted lines<br/>
  
==What machine does a remote job execute on?==
----
By default Qizmt determines what machine a remote job runs on, however this can be overridden explicitly by specifying `<Host>`  in the DFS_IO section of the remote job.


If redundancy is enabled, then any job can execute on any machine if not explicitly specified.  Ideally can use network paths always so that it doesn’t matter what machine a job executes on. E.g. in the case of redundancy factor is > 1, all job types will failover to a good machine if a machine is inaccessible.

==Can I explicitly specify what machine a local job runs on?==
----
Yes, can also specify an explicit host for a local job:
{{{
<SourceCode>
  <Jobs>
    <Job Name="ExplicitLocalHost" Custodian="" Email="">
      <IOSettings>
        <JobType>local</JobType>
        <LocalHost>localhost</LocalHost>
      </IOSettings>
      <Local>
      <![CDATA[
      public virtual void Local()
      {
        Qizmt_Log("This local job is running from " + Qizmt_MachineHost);
      }
      ]]>
      </Local>
   </Job>
  </Jobs>
</SourceCode>
}}}

==How to execute a remote job on all machines or all cores without explicitly listing a series of `<DFS_IO>`s?==
----
{{{
<DFS_IO_Multi>
  <DFSReader>dfs://Qizmt-RemoteMultiIO_Input.txt</DFSReader>
  <DFSWriter>dfs://Qizmt-RemoteMultiIO_Output1####.txt</DFSWriter>
  <Mode>ALL MACHINES</Mode>
</DFS_IO_Multi>
<DFS_IO_Multi>
  <DFSReader>dfs://Qizmt-RemoteMultiIO_Input.txt</DFSReader>
  <DFSWriter>dfs://Qizmt-RemoteMultiIO_Output2####.txt</DFSWriter>
  <Mode>ALL CORES</Mode>
</DFS_IO_Multi>
}}}

This way you don’t have to explicitly specify every machine or every core if you want a remote job to run everywhere.

==How to synchronize all remote, map or reduce processes on a single machine?==
----
If you want to use a OS wide critical section you can use:
{{{
System.Threading.Mutex osCriticalSection = new System.Threading.Mutex(false,
"CreatePublishDir{7104700F-424F-4a34-BAF8-6A127852A216}");
osCriticalSection.WaitOne();
{
  sw.WriteLine(val);
}
osCriticalSection.ReleaseMutex();
osCriticalSection.Close();
}}}

But it would probably be more efficient to just use a different filename for each reducer process by appending Qizmt_ProcessID.ToString() to the file name.

Here are qizmt variables in case it helps with coordinating:

{{{int Qizmt_ProcessID}}} – id of current process in cluster, 0 to Qizmt_ProcessCount - 1

  
{{{int Qizmt_ProcessCount}}} – total of number of processes for the job, usually equal to number of                                                  cores or a prime near the number of cores

                                                       
{{{string Qizmt_MachineHost}}} – hostname of current machine that current mapper, reducer, local or remote is running on

                                                            
{{{string Qizmt_MachineIP}}} – current machine that current mapper, reducer, local or remote is running on

  
{{{string[] Qizmt_ExecArgs}}} – commandline arguments sent in with the qizmt exec command


e.g. if there are 4000 cores in the cluster, the index created from reducer with Qizmt_ProcessID 0 will have the first sorted range, the file created from reducer with Qizmt_ProcessID 1 will have the second sorted range all the way up to Qizmt_ProcessID 3999 which will have the last sorted range.

==How to copy files into and out of MR.DFS from shares which I have access to but the account Qizmt is running under does not?==
----
You can use rget and rput to use the credentials of the account issuing the command to move files between MR.DFS and a share somewhere.


*qizmt rput* – copies the file from the share to the local machine using the executing users credentials, qizmt rput then copies the file into MR.DFS using the qizmt credentials.


*qizmt rget* – copies the file from MR.DFS using the qizmt credentials to a temp directory on the current machine, qizmt rget then copies the file from the temp directory to the share using the executing users credentials.


Alternatively, you can use regular *qizmt get* to get the file out of MR.DFS and then copy it to the network share with regular windows 
*copy /Y*


Alternatively, can use a remote job to stream the file somewhere with some kind of API such as ADO.Net


If you want to use get to get the file but it is too big to fit on single machine you can get it out in parts:


{{{Qizmt get [parts=<first>[-[<last>]]] <dfspath> <netpath>}}}


The total number of parts can be seen in the *qizmt ls* command.


{{{Qizmt info <dfsfilename>}}} will show how many parts on each machine
{{{Qizmt info <dfsfilename>:<host>}}} will show how many parts on each machine and the size of each part.


The size of each parts caps around 64 megabytes


Can use *qizmt info* to see how many parts a MR.DFS file has:


C:\>Qizmt info datafile.txt

[DFS file information]

 DFS File: datafile.txt<br/>
  Size: 4 TB (4398046511104)<br/>
  Sample Size: 467.27 KB (116.82 KB avg)<br/>
  Parts: 9351<br/>
    X parts on MACHINEA (500 GB data; 467.27 KB samples)<br/>
    X parts on MACHINEB (500 GB data; 467.27 KB samples)<br/>
    X parts on MACHINEC (500 GB data; 467.27 KB samples)<br/>
    X parts on MACHINED (500 GB data; 467.27 KB samples)<br/>
    X parts on MACHINEE (500 GB data; 467.27 KB samples)<br/>
    X parts on MACHINEF (500 GB data; 467.27 KB samples)<br/>
    X parts on MACHINEG (500 GB data; 467.27 KB samples)<br/>
    X parts on MACHINEG (500 GB data; 467.27 KB samples)<br/>


Can use the parts switch on qizmt get to get ranges of parts if they do not all fit on a single machine:


qizmt get parts=0-999 datafile.txt \\MACHINEA\Drop\temp.txt

copy /Y \\MACHINEA\Drop\temp.txt \\TARGET0\Drop\temp.txt

del /Q /P \\MACHINEA\Drop\temp.txt

qizmt get parts=1000-1999 datafile.txt \\MACHINEA\Drop\temp.txt

copy /Y \\MACHINEA\Drop\temp.txt \\TARGET1\Drop\temp.txt

del /Q /P \\MACHINEA\Drop\temp.txt

qizmt get parts=2000-2999 datafile.txt \\MACHINEA\Drop\temp.txt

copy /Y \\MACHINEA\Drop\temp.txt \\TARGET2\Drop\temp.txt

del /Q /P \\MACHINEA\Drop\temp.txt

qizmt get parts=3000-3999 datafile.txt \\MACHINEA\Drop\temp.txt

copy /Y \\MACHINEA\Drop\temp.txt \\TARGET3\Drop\temp.txt

del /Q /P \\MACHINEA\Drop\temp.txt

qizmt get parts=4000-4999 datafile.txt \\MACHINEA\Drop\temp.txt

copy /Y \\MACHINEA\Drop\temp.txt \\TARGET4\Drop\temp.txt

del /Q /P \\MACHINEA\Drop\temp.txt

qizmt get parts=5000-5999 datafile.txt \\MACHINEA\Drop\temp.txt

copy /Y \\MACHINEA\Drop\temp.txt \\TARGET5\Drop\temp.txt

del /Q /P \\MACHINEA\Drop\temp.txt

qizmt get parts=6000-6999 datafile.txt \\MACHINEA\Drop\temp.txt

copy /Y \\MACHINEA\Drop\temp.txt \\TARGET6\Drop\temp.txt

del /Q /P \\MACHINEA\Drop\temp.txt

qizmt get parts=7000-7999 datafile.txt \\MACHINEA\Drop\temp.txt

copy /Y \\MACHINEA\Drop\temp.txt \\TARGET7\Drop\temp.txt

del /Q /P \\MACHINEA\Drop\temp.txt

qizmt get parts=8000-8999 datafile.txt \\MACHINEA\Drop\temp.txt

copy /Y \\MACHINEA\Drop\temp.txt \\TARGET8\Drop\temp.txt

del /Q /P \\MACHINEA\Drop\temp.txt

qizmt get parts=9000-9351 datafile.txt \\MACHINEA\Drop\temp.txt

copy /Y \\MACHINEA\Drop\temp.txt \\TARGET9\Drop\temp.txt

del /Q /P \\MACHINEA\Drop\temp.txt

Thought if sending a large amount of data to a bunch of machines, it may be advantageous to install a qizmt cluster on those machines and have a mapreducer job just dump the data out locally on that cluster so that it is not bottlenecked.

==Error: Unable to check user logs on ‘SOMEHOST’==
----
It looks like there is too much going tout to Qizmt_Log() this is only for debugging not for transporting gigabytes of data. Can fix by throttling to only let each reduce or mapper log up to a handful of Qizmt_Log() calls.


Could do something like:
{{{
public int logfive = 5;
public virtual void Map(ByteSlice line, MapOutput output)
{
  .
  .
  .
  if(error_condition)
  {
    if(logfive > 0)
    {
      Qizmt_Log(someerror);
    }
    --logfive;
  }
}}}
Or, you can configure qizmt to allow only a maximum number of Qizmt_Log calls for each job type by:  
*qizmt maxuserlogsupdate `<integer>`*


If there are millions of error conditions in the data that you need to list, then don’t use Qizmt_Log but rather can make a separate mapreducer to find these.
{{{
public virtual void Map(ByteSlice line, MapOutput output)
  {
    .
    .
    .
    if(error_condition)
    {
      output.add(… some error in data
    }
}}}

==How to do DESC or descending sort of mapreducer jobs?==
----
This can be achieved by setting the OutputDirection to descending in IOSettings:
{{{
  <OutputMethod>sorted</OutputMethod>
  <OutputDirection>descending</OutputDirection>
</IOSettings> 
}}}

==Is there a global namespace?  I.e., if I want to define a field delimiter constant and make it available to a 'remote' job and both members of a 'mapreduce' job, how would I do this?==
----
Global can be created and muted with DGlobals.Add(“`<name>`”, “`<value>`”); in local jobs only. And are read only in subsequent remote and mapreduce jobs.


Another option for sharing information between jobs which can be used to share mutable files between all job types is to use a global critical section, however this is a critical section shared by all processes in the cluster so should be used sparingly, e.g. on first map iteration, reduceinitialize and reducefinalize. 


There is example in built in examples: Qizmt-ClusterLock.xml
{{{
using(GlobalCriticalSection.GetLock())
{  
  //cluster wide critical section
}
}}}

==Is it possible to use byteslice as a key? ==
----
Yes you can use byteslice as a key, but the size must match key length. byteslice can wrap a regular {{{byte[]}}} or {{{List<byte>}}}


The _Qizmt Reference Guide_ has all available API's with examples of using each. 

|| {{{public void CopyTo(byte[] myBuffer)}}}<br/>Copies this buffer into myBuffer. ||
|| *Remarks*<br/>This is a more efficient version of the ToBytes() method.  A {{{byte[]}}} is passed in to retrieve the buffer, rather than allocating a brand new byte array every time. ||

*Example*

In this example, a byte array myBuffer is allocated outside of the Reduce method.   This byte array will last for the lifetime of the slave.  Slaves are single threaded.
The Qizmt_KeyLength constant is used here during the allocation of the byte array.  This ensures that the byte array is large enough when the key buffer is copied into it.  
{{{
//Allocate myBuffer outside of the Reduce method.
byte[] myBuffer = new Byte[Qizmt_KeyLength];

public override void Reduce(ByteSlice key, RandomAccessEntries values, RandomAccessOutput output)
{
  key.CopyTo(myBuffer);
  //Examine the first byte of key.
  //key[0] works also.
  if (myBuffer[0] == 0x0)
  {                        
    string sKey = UnpadKey(key).ToString();
    output.Add(ByteSlice.Prepare("key=" + sKey));
  }
  else
  {
     //Do something else...
  }
}
}}}


|| {{{public void CopyTo(byte[] myBuffer, int myBufferOffset)}}}<br/>Copies this buffer into myBuffer starting at myBufferOffset. ||
|| *Remarks*<br/>This is a more efficient version of the ToBytes() method.  A {{{byte[]}}} is passed in to retrieve the buffer, rather than allocating a brand new byte array every time. ||

*Example*
{{{
//Allocate myBuffer outside of the Reduce method.
byte[] myBuffer = new Byte[16];

public override void Reduce(ByteSlice key, RandomAccessEntries values, RandomAccessOutput output)
{
  key.CopyTo(myBuffer, 0);

  //Examine the first byte of key.
  if (myBuffer[0] == 0x0)
  {
    string sKey = UnpadKey(key).ToString();
    output.Add(ByteSlice.Prepare("key=" + sKey));
  }
  else
  {
    //Do something else...
  }
}
</source>
}}}


||{{{public void AppendTo(List<byte> list)}}}<br/>Appends this ByteSlice’s buffer to list. ||
||*Remarks*<br/>{{{List<byte>}}} is usually the easiest way to work with binary. ||

*Example*
{{{
List<byte> myBuffer = new List<byte>();
public override void Reduce(ByteSlice key, RandomAccessEntries values, RandomAccessOutput output)
{
  myBuffer.Clear();
  key.AppendTo(myBuffer);

  if (myBuffer[0] == 0x61)
  {                        
    string sKey = UnpadKey(key).ToString();
    output.Add(ByteSlice.Prepare("key=" + sKey));
  }
  else
  {
    //Do something else.
   }
}
</source>
}}}

==I have a need to use a binary tree for lookups in the map portion of a mapreduce job.  Is it possible to reference external DLLs in Qizmt?  It would be for read-only purposes, and could thus be a global variable.==
----
Yes, this is in Quck  Start Guide. There is also example in qizmt examples command: Qizmt-AddUsingReferences.xml


*Referencing a Custom .Net DLL in a Mapreducer*

If you want to build a C# DLL and reference it, you can build a regular .net DLL and use *Qizmt put* to put it into MR.DFS then reference it in the job. Qizmt put automatically treats `*`.dll files differently as other files and makes a redundant copy of the DLL on every machine in the cluster. After putting a DLL into a Qizmt cluster, it can be referenced from within the job:

{{{
<Job Name="testdll" Custodian="" email="">
  <Add Reference="Qizmt-AddUsingReferences_testdll.dll" Type="dfs" /> <!- custom C# DLL put into MR.DFS ->
  <Add Reference="Microsoft.VisualBasic.dll" Type="system" /> <!-- system DLL already in .NET -->
  <Using>testdll</Using> <!-- using namespace -->
  <Using>Microsoft.VisualBasic</Using><!-- using namespace -->
  .
  .
  .
}}}


*qizmt put `*`.dll* puts a dll into MR.DFS and makes it available to all job types; local, remote and mapreduce. There are two reference types supported *dfs* and *system*.


Can use type *dfs* for custom DLLs or DLLs that are referenced by custom DLLs used in local, remote or mapreducer jobs.
Can use type *system* for referencing system DLLs that are not referenced by default. Here are the system DLLs that are referenced by default:

  System.dll
  System.Xml.dll
  System.Data.dll    
  System.Drawing.dll
  System.Core.dll


e.g.

{{{
<Add Reference="regression_test_testdll.dll" Type="dfs" />
<Add Reference="Microsoft.VisualBasic.dll" Type="system" />
<MapReduce>
  <Map>
  <![CDATA[
  public virtual void Map(ByteSlice line, MapOutput output)
  {
    if(32 != Microsoft.VisualBasic.Strings.Asc(' '))
    {
      throw new Exception("Map: (32 != Microsoft.VisualBasic.Strings.Asc(' '))");
    }
    //...
  }
  ]]>
}}}

==How to stream multiple files in other VLANs into a Qizmt cluster?==
----
Data can be streamed into a cluster in without being limited to the disk space of a single machine is to create a remote job and stream the data directly into a MR.DFS file, this will also avoid extra disk copying, however this will require the Qizmt account to have read permissions set on the source file or files.
{{{
<SourceCode>
  <Jobs>
    <Job Name="StreamBigFileIn_CreateSampleData" Custodian="" Email="" Description="Create sample data">
      <IOSettings>
        <JobType>remote</JobType>
        <DFS_IO>
          <DFSReader></DFSReader>
          <DFSWriter>dfs://BigFile.txt</DFSWriter>
        </DFS_IO>
      </IOSettings>
      <Remote>
        <![CDATA[
          public virtual void Remote(RemoteInputStream dfsinput, RemoteOutputStream dfsoutput)
          {             
                string sLine; //if binary data with fixed length tuples, use byte[] wrapped in byteslice then put into recordset before outputing
                using(System.IO.StreamReader sr = new System.IO.StreamReader("\\NETWORKPATH..."))
                {
                    //stream line at a time or chunk at a time, how ever the data is formatted
                    .
                    .
                    .
                    dfsoutput.WriteLine(sline); 
                }
           }
        ]]>
      </Remote>
    </Job>
  </Jobs>
</SourceCode>
}}}

==mstring is memory-managed by qizmt server?==
----
YES

==mstring avoids allocating heap space for every pass thru Map(...) and/or Reduce(...)?==
----
YES, unless an iteration uses more memory, then it doubles just for that iteration

==mstring is encoded as UTF-16?==
----
YES

==KeyLength of an mstring containing chars that could be encoded in ASCII will thus be 2*mstring.Length?==
----
YES, because mstring stores strings in utf16 the keylengh must be set to 2 times the expected string length

==I have a mapreduce job, part of whose output is the count of values for a given key.  I would like to be able to sort my output based, in part, on this count.  Is there any way to do this, without running the data through another mapreduce job where the count is part of the key?  It seems to be overkill, as there would be no reduction, just new mapping.==
----
This would have to be two mapreducers because any server could contribute to the count for any word so there is nothing to sort on until the count of each word comprehensively considers all words on all machines.

However, you can use mapreducer caching on both the word counter and the count sorter so that only delta gets exchanged/sorted each time you execute it.
 
<img src="http://qizmt.googlecode.com/svn/wiki/images/Qizmt_WordCountExample.png" alt="Qizmt Word Count Example" />

Qizmt-ExplicitCacheWordCount.xml is in the built-in examples and shows word count which only exchanges/sorts new texts in the cluster but producing a comprehensive word count after each iteration.